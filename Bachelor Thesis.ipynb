{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c428b192",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class EZReaderDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Loads the merged EZ-Reader + Provo data and returns one sentence at a time.\n",
    "    Expects a CSV with columns:\n",
    "      sentence_id, word_in_sentence,\n",
    "      freq_z, len_z, cloze_z,\n",
    "      SFD_z, FFD_z, GD_z, TT_z, GP_z,\n",
    "      PrF, Pr1, Pr2, PrS\n",
    "    \"\"\"\n",
    "    def __init__(self, path_csv, device=device):\n",
    "        df = pd.read_csv(path_csv)\n",
    "       \n",
    "        self.sent_ids = sorted(df['sentence_id'].unique())\n",
    "        self.groups = {\n",
    "            sid: df[df['sentence_id']==sid]\n",
    "                     .sort_values('word_in_sentence')\n",
    "                     for sid in self.sent_ids\n",
    "        }\n",
    "        self.device = device\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sent_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sid = self.sent_ids[idx]\n",
    "        g = self.groups[sid]\n",
    "        # inputs: freq_z, len_z, cloze_z\n",
    "        X = torch.tensor(\n",
    "            g[['freq_z','len_z','cloze_z']].values,\n",
    "            dtype=torch.float32,\n",
    "            device=self.device\n",
    "        )            \n",
    "        # continuous outputs: SFD_z, FFD_z, GD_z, TT_z, GP_z\n",
    "        y_cont = torch.tensor(\n",
    "            g[['SFD_z','FFD_z','GD_z','TT_z','GP_z']].values,\n",
    "            dtype=torch.float32,\n",
    "            device=self.device\n",
    "        )            \n",
    "        # probabilistic outputs: PrF, Pr1, Pr2, PrS\n",
    "        y_prob = torch.tensor(\n",
    "            g[['PrF','Pr1','Pr2','PrS']].values,\n",
    "            dtype=torch.float32,\n",
    "            device=self.device\n",
    "        )            \n",
    "        return X, y_cont, y_prob\n",
    "\n",
    "def collate_sentences(batch):\n",
    "    \"\"\"\n",
    "    Pads a batch of (X, y_cont, y_prob) by sequence length with zeros,\n",
    "    and returns lengths on the same device as X.\n",
    "    \"\"\"\n",
    "\n",
    "    Xs, Yc, Yp = zip(*batch)\n",
    "   \n",
    "    device = Xs[0].device\n",
    "\n",
    "    lengths = torch.tensor([x.size(0) for x in Xs],\n",
    "                           dtype=torch.long,\n",
    "                           device=device)\n",
    "\n",
    "    T_max = lengths.max().item()\n",
    "\n",
    "    def pad(seqs, dim):\n",
    "        padded = torch.zeros(len(seqs), T_max, dim, device=device)\n",
    "        for i, seq in enumerate(seqs):\n",
    "            padded[i, : seq.size(0), :] = seq\n",
    "        return padded\n",
    "\n",
    "    X_pad  = pad(Xs, 3)\n",
    "    yc_pad = pad(Yc, 5)\n",
    "    yp_pad = pad(Yp, 4)\n",
    "    return X_pad, yc_pad, yp_pad, lengths\n",
    "\n",
    "\n",
    "class EZReaderRNN(nn.Module):\n",
    "    \"\"\"\n",
    "    RNN that mimics EZ-Reader’s serial processing:\n",
    "     - uni-directional LSTM over word-by-word inputs\n",
    "     - two heads: regression for durations, sigmoid‐prob for fixation‐probs\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 input_size=3,\n",
    "                 hidden_size=256,\n",
    "                 num_layers=1,\n",
    "                 cont_out=5,\n",
    "                 prob_out=4,\n",
    "                 dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size,\n",
    "                            hidden_size,\n",
    "                            num_layers,\n",
    "                            batch_first=True,\n",
    "                            dropout=dropout)\n",
    "        \n",
    "        self.reg_head = nn.Linear(hidden_size, cont_out)\n",
    "        \n",
    "        self.prob_head = nn.Sequential(\n",
    "            nn.Linear(hidden_size, prob_out),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(\n",
    "            x, lengths.cpu(), batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "        packed_out, _ = self.lstm(packed)\n",
    "        out, _ = nn.utils.rnn.pad_packed_sequence(\n",
    "            packed_out, batch_first=True\n",
    "        ) \n",
    "\n",
    "        \n",
    "        y_cont = self.reg_head(out)     \n",
    "        y_prob = self.prob_head(out)   \n",
    "        return y_cont, y_prob\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    ds = EZReaderDataset('ezreader_training_data_with_ids.csv', device=device)\n",
    "    dl = DataLoader(ds,\n",
    "                    batch_size=16,\n",
    "                    shuffle=True,\n",
    "                    collate_fn=collate_sentences)\n",
    "\n",
    "    model = EZReaderRNN().to(device)\n",
    "    # define losses & optimizer\n",
    "    mse_loss   = nn.MSELoss()\n",
    "    bce_loss   = nn.BCELoss()\n",
    "    optimizer  = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "  \n",
    "    for Xb, Yc_b, Yp_b, Lb in dl:\n",
    "        optimizer.zero_grad()\n",
    "        pred_cont, pred_prob = model(Xb, Lb)\n",
    "\n",
    "\n",
    "        device = pred_cont.device\n",
    "        seq_len = pred_cont.size(1)\n",
    "        mask = (torch.arange(seq_len, device=device)[None, :]\n",
    "                < Lb[:, None]).float().unsqueeze(-1)\n",
    "\n",
    "        loss_cont = mse_loss(pred_cont * mask, Yc_b * mask)\n",
    "        loss_prob = bce_loss(pred_prob * mask, Yp_b * mask)\n",
    "        (loss_cont + loss_prob).backward()\n",
    "        optimizer.step()\n",
    "        print(f\"batch loss: {loss_cont.item() + loss_prob.item():.4f}\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65961f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "full_ds = EZReaderDataset('ezreader_training_data_with_ids.csv', device=device)\n",
    "n = len(full_ds)\n",
    "n_train = int(0.8 * n)\n",
    "n_val   = int(0.1 * n)\n",
    "n_test  = n - n_train - n_val\n",
    "\n",
    "train_ds, val_ds, test_ds = random_split(full_ds, [n_train, n_val, n_test])\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=16, shuffle=True,  collate_fn=collate_sentences)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=16, shuffle=False, collate_fn=collate_sentences)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=16, shuffle=False, collate_fn=collate_sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c9fa24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    all_true_cont, all_pred_cont = [], []\n",
    "    all_true_prob, all_pred_prob = [], []\n",
    "    with torch.no_grad():\n",
    "        for Xb, Yc_b, Yp_b, Lb in loader:\n",
    "            pred_c, pred_p = model(Xb, Lb)\n",
    "            device = pred_c.device\n",
    "\n",
    "            # build mask\n",
    "            seq_len = pred_c.size(1)\n",
    "            mask = (torch.arange(seq_len, device=device)[None, :]\n",
    "                    < Lb[:,None]).unsqueeze(-1)\n",
    "\n",
    "            # extract only valid timesteps\n",
    "            true_c = (Yc_b * mask).reshape(-1, pred_c.size(2))[mask.reshape(-1)]\n",
    "            pred_c = (pred_c * mask).reshape(-1, pred_c.size(2))[mask.reshape(-1)]\n",
    "            true_p = (Yp_b * mask).reshape(-1, pred_p.size(2))[mask.reshape(-1)]\n",
    "            pred_p = (pred_p * mask).reshape(-1, pred_p.size(2))[mask.reshape(-1)]\n",
    "\n",
    "            all_true_cont.append(true_c.cpu().numpy())\n",
    "            all_pred_cont.append(pred_c.cpu().numpy())\n",
    "            all_true_prob.append(true_p.cpu().numpy())\n",
    "            all_pred_prob.append(pred_p.cpu().numpy())\n",
    "\n",
    "    true_cont = np.vstack(all_true_cont)\n",
    "    pred_cont = np.vstack(all_pred_cont)\n",
    "    true_prob = np.vstack(all_true_prob)\n",
    "    pred_prob = np.vstack(all_pred_prob)\n",
    "\n",
    "    # Continuous metrics\n",
    "    rmse = np.sqrt(mean_squared_error(true_cont.flatten(), pred_cont.flatten()))\n",
    "    r, _  = pearsonr(true_cont.flatten(), pred_cont.flatten())\n",
    "\n",
    "    # Probabilities as regression\n",
    "    mse_prob = mean_squared_error(true_prob.flatten(), pred_prob.flatten())\n",
    "\n",
    "    return rmse, r, mse_prob\n",
    "\n",
    "\n",
    "# 1. Initialize checkpoint variables\n",
    "best_val_rmse = float('inf')\n",
    "best_epoch   = -1\n",
    "\n",
    "# 2. Training + validation loop\n",
    "num_epochs = 50\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    # --- training ---\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    for Xb, Yc_b, Yp_b, Lb in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        pred_c, pred_p = model(Xb, Lb)\n",
    "        device = pred_c.device\n",
    "        seq_len = pred_c.size(1)\n",
    "        mask = (torch.arange(seq_len, device=device)[None, :] < Lb[:,None]) \\\n",
    "                   .float().unsqueeze(-1)\n",
    "\n",
    "        loss_c = mse_loss(pred_c * mask, Yc_b * mask)\n",
    "        loss_p = bce_loss(pred_p * mask, Yp_b * mask)\n",
    "        loss = loss_c + loss_p\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "    avg_train_loss = np.mean(train_losses)\n",
    "\n",
    "    # --- evaluation on validation set ---\n",
    "    model.eval()\n",
    "    all_true_cont, all_pred_cont = [], []\n",
    "    all_true_prob, all_pred_prob = [], []\n",
    "    with torch.no_grad():\n",
    "        for Xb, Yc_b, Yp_b, Lb in val_loader:\n",
    "            pred_c, pred_p = model(Xb, Lb)\n",
    "            device = pred_c.device\n",
    "            seq_len = pred_c.size(1)\n",
    "            mask = (torch.arange(seq_len, device=device)[None, :] < Lb[:,None]) \\\n",
    "                       .unsqueeze(-1)\n",
    "\n",
    "            # flatten valid timesteps\n",
    "            true_c = (Yc_b * mask).reshape(-1, pred_c.size(2))[mask.reshape(-1)]\n",
    "            pred_c = (pred_c * mask).reshape(-1, pred_c.size(2))[mask.reshape(-1)]\n",
    "            true_p = (Yp_b * mask).reshape(-1, pred_p.size(2))[mask.reshape(-1)]\n",
    "            pred_p = (pred_p * mask).reshape(-1, pred_p.size(2))[mask.reshape(-1)]\n",
    "\n",
    "            all_true_cont.append(true_c.cpu().numpy())\n",
    "            all_pred_cont.append(pred_c.cpu().numpy())\n",
    "            all_true_prob.append(true_p.cpu().numpy())\n",
    "            all_pred_prob.append(pred_p.cpu().numpy())\n",
    "\n",
    "    true_cont = np.vstack(all_true_cont)\n",
    "    pred_cont = np.vstack(all_pred_cont)\n",
    "    true_prob = np.vstack(all_true_prob)\n",
    "    pred_prob = np.vstack(all_pred_prob)\n",
    "\n",
    "    # compute metrics\n",
    "    val_rmse     = np.sqrt(mean_squared_error(true_cont.flatten(), pred_cont.flatten()))\n",
    "    val_r, _     = pearsonr(true_cont.flatten(), pred_cont.flatten())\n",
    "    val_mseprob  = mean_squared_error(true_prob.flatten(), pred_prob.flatten())\n",
    "\n",
    "    print(f\"Epoch {epoch:02d} | train loss {avg_train_loss:.4f}\"\n",
    "          f\" | val RMSE {val_rmse:.4f}, r {val_r:.3f}, prob-MSE {val_mseprob:.4f}\")\n",
    "\n",
    "    # 3. Checkpoint \n",
    "    if val_rmse < best_val_rmse:\n",
    "        best_val_rmse = val_rmse\n",
    "        best_epoch    = epoch\n",
    "        torch.save(model.state_dict(), 'best_model.pt')\n",
    "        \n",
    "\n",
    "# 4. After training\n",
    "print(f\"\\nTraining complete. Best validation RMSE {best_val_rmse:.4f} at epoch {best_epoch}.\")\n",
    "print(\"Best model saved to best_model.pt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00448bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate and load the trained model\n",
    "model = EZReaderRNN().to(device)\n",
    "model.load_state_dict(torch.load('best_model.pt', map_location=device))\n",
    "model.eval()\n",
    "\n",
    "# === Evaluation on Test Set ===\n",
    "\n",
    "all_true_c, all_pred_c = [], []\n",
    "all_true_p, all_pred_p = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for Xb, Yc_b, Yp_b, Lb in test_loader:\n",
    "        pc, pp = model(Xb, Lb)\n",
    "        device = pc.device\n",
    "        seq_len = pc.size(1)\n",
    "        mask = (torch.arange(seq_len, device=device)[None, :] < Lb[:,None]).unsqueeze(-1)\n",
    "\n",
    "        true_c = (Yc_b * mask).reshape(-1, pc.size(2))[mask.reshape(-1)]\n",
    "        pred_c = (pc   * mask).reshape(-1, pc.size(2))[mask.reshape(-1)]\n",
    "        true_p = (Yp_b * mask).reshape(-1, pp.size(2))[mask.reshape(-1)]\n",
    "        pred_p = (pp   * mask).reshape(-1, pp.size(2))[mask.reshape(-1)]\n",
    "\n",
    "        all_true_c.append(true_c.cpu().numpy())\n",
    "        all_pred_c.append(pred_c.cpu().numpy())\n",
    "        all_true_p.append(true_p.cpu().numpy())\n",
    "        all_pred_p.append(pred_p.cpu().numpy())\n",
    "\n",
    "true_c = np.vstack(all_true_c)\n",
    "pred_c = np.vstack(all_pred_c)\n",
    "true_p = np.vstack(all_true_p)\n",
    "pred_p = np.vstack(all_pred_p)\n",
    "\n",
    "# Metrics\n",
    "rmse = np.sqrt(mean_squared_error(true_c.flatten(), pred_c.flatten()))\n",
    "r, _ = pearsonr(true_c.flatten(), pred_c.flatten())\n",
    "prob_mse = mean_squared_error(true_p.flatten(), pred_p.flatten())\n",
    "\n",
    "print(f\"Test RMSE (durations, z‐scores): {rmse:.4f}\")\n",
    "print(f\"Test Pearson's r (durations):    {r:.4f}\")\n",
    "print(f\"Test MSE (probabilities):         {prob_mse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d2f6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# === Paths ===\n",
    "PROVO_CSV    = 'Provo_Corpus-Eyetracking_Data.csv'\n",
    "TRAIN_CSV    = 'ezreader_training_data_with_ids.csv'\n",
    "BEST_MODEL   = 'best_model.pt'\n",
    "OUT_CSV      = 'RNN_Word-Based_Means.csv'\n",
    "\n",
    "# === 1. compute Provo aggregates\n",
    "df_p = pd.read_csv(PROVO_CSV)\n",
    "agg = (\n",
    "    df_p.groupby(['Text_ID','Word_In_Sentence_Number'])\n",
    "        .agg(\n",
    "            SFD=('IA_DWELL_TIME',              lambda x: x[df_p.loc[x.index,'IA_FIXATION_COUNT']==1].mean()),\n",
    "            FFD=('IA_FIRST_FIXATION_DURATION', 'mean'),\n",
    "            GD=('IA_FIRST_RUN_DWELL_TIME',     'mean'),\n",
    "            TT=('IA_DWELL_TIME',               'mean'),\n",
    "            GP=('IA_RUN_COUNT',                lambda x: np.mean(x>0)),\n",
    "        )\n",
    "        .reset_index()\n",
    "        .rename(columns={'Text_ID':'sentence_id',\n",
    "                         'Word_In_Sentence_Number':'word_in_sentence'})\n",
    ")\n",
    "agg = agg.sort_values(['sentence_id','word_in_sentence']).reset_index(drop=True)\n",
    "\n",
    "# Fit scaler on continuous ground truth\n",
    "cont_cols = ['SFD','FFD','GD','TT','GP']\n",
    "scaler_y_cont = StandardScaler().fit(agg[cont_cols].values)\n",
    "\n",
    "# === 2. Load simulation map for words and positions ===\n",
    "\n",
    "sim_file = 'simulation_corpus_final.txt'\n",
    "\n",
    "# Load simulation corpus\n",
    "sim_cols = ['frequency','length','cloze','word_raw']\n",
    "df_sim = pd.read_csv(sim_file, delim_whitespace=True, names=sim_cols, engine='python')\n",
    "df_sim['is_end'] = df_sim['word_raw'].str.endswith('@')\n",
    "df_sim['word'] = df_sim['word_raw'].str.rstrip('@')\n",
    "df_sim['sentence_id'] = df_sim['is_end'].cumsum().astype(int)\n",
    "df_sim['word_in_sentence'] = df_sim.groupby('sentence_id').cumcount() + 1\n",
    "\n",
    "\n",
    "df_map = df_sim[['sentence_id','word_in_sentence','word']]\n",
    "\n",
    "# === 3. Load training CSV for inference\n",
    "df_train = pd.read_csv(TRAIN_CSV)\n",
    "\n",
    "groups = {\n",
    "    sid: grp.sort_values('word_in_sentence')\n",
    "    for sid, grp in df_train.groupby('sentence_id')\n",
    "}\n",
    "\n",
    "# === 4. Load model ===\n",
    "class EZReaderRNN(nn.Module):\n",
    "    def __init__(self, input_size=3, hidden_size=256, num_layers=1,\n",
    "                 cont_out=5, prob_out=4, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers,\n",
    "                            batch_first=True, dropout=dropout)\n",
    "        self.reg_head = nn.Linear(hidden_size, cont_out)\n",
    "        self.prob_head = nn.Sequential(nn.Linear(hidden_size, prob_out), \n",
    "                                       nn.Sigmoid())\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(\n",
    "            x, lengths.cpu(), batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "        out_pack, _ = self.lstm(packed)\n",
    "        out, _ = nn.utils.rnn.pad_packed_sequence(out_pack, batch_first=True)\n",
    "        return self.reg_head(out), self.prob_head(out)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = EZReaderRNN().to(device)\n",
    "model.load_state_dict(torch.load(BEST_MODEL, map_location=device))\n",
    "model.eval()\n",
    "\n",
    "# === 5. Inference ===\n",
    "pred_cont_z = []\n",
    "pred_prob = []\n",
    "sid_list = []\n",
    "wordpos_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for sid, grp in sorted(groups.items()):\n",
    "       \n",
    "        X = torch.tensor(grp[['freq_z','len_z','cloze_z']].values,\n",
    "                         dtype=torch.float32, device=device)\n",
    "        lengths = torch.tensor([X.size(0)], dtype=torch.long, device=device)\n",
    "        X = X.unsqueeze(0) \n",
    "        # Forward pass\n",
    "        pc_z, pp = model(X, lengths)\n",
    "        pc_z = pc_z.squeeze(0).cpu().numpy()  \n",
    "        pp = pp.squeeze(0).cpu().numpy()      \n",
    "        # Record predictions\n",
    "        for i in range(pc_z.shape[0]):\n",
    "            sid_list.append(sid)\n",
    "            wordpos_list.append(int(grp.iloc[i]['word_in_sentence']))\n",
    "            pred_cont_z.append(pc_z[i])\n",
    "            pred_prob.append(pp[i])\n",
    "\n",
    "# Convert lists to arrays\n",
    "pred_cont_z = np.vstack(pred_cont_z)  \n",
    "pred_prob = np.vstack(pred_prob)     \n",
    "\n",
    "# 6. Inverse transform continuous predictions to ms\n",
    "pred_cont = scaler_y_cont.inverse_transform(pred_cont_z)\n",
    "\n",
    "# 7. Assemble output DataFrame\n",
    "out_df = pd.DataFrame(pred_cont, columns=cont_cols)\n",
    "prob_cols = ['PrF','Pr1','Pr2','PrS']\n",
    "for idx, col in enumerate(prob_cols):\n",
    "    out_df[col] = pred_prob[:, idx]\n",
    "\n",
    "out_df['sentence_id'] = sid_list\n",
    "out_df['word_in_sentence'] = wordpos_list\n",
    "\n",
    "# 8. Merge with word map to get the actual word strings\n",
    "out_df = out_df.merge(df_map, on=['sentence_id','word_in_sentence'])\n",
    "\n",
    "# 9. Reorder columns\n",
    "cols = ['word','sentence_id','word_in_sentence'] + cont_cols + prob_cols\n",
    "out_df = out_df[cols]\n",
    "\n",
    "# 10. Save to CSV\n",
    "out_df.to_csv(OUT_CSV, index=False)\n",
    "print(f\"Saved word-based means CSV to {OUT_CSV}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
